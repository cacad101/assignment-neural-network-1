{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment aims to provide you with some exposure to the use of neural networks for regression/approximation problems. Download the California Housing database:\n",
    "\n",
    "- https://sites.google.com/site/sukritsite/teaching\n",
    "- http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
    "\n",
    "This database contains attributes of housing complexes in California such as location, dimensions, etc., together with their corresponding prices. The aim is to predict the housing prices in the test dataset after training the neural networks on other attributes in the training data. Read the data from the file ‘california_housing.data’.\n",
    "\n",
    "This is a model selection task, i.e. you have to try out different parameters for the neural network, forming different models and select the one which gives you highest accuracy. Divide the data into test and train set at a ratio of 0.3: 0.7. You need to scale and normalise only the input features in the data to zero mean and unit standard deviation. You need to perform 5-fold cross-validation on the train data in order to select the best models. For this, you will further divide the train data into five folds.\n",
    "\n",
    "Each data sample is a row of 9 values: 8 input attributes and median housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the model first we need to preprocess the data. The following is the code to preprocess the `cal_housing.data`.\n",
    "Start with the utility used by the Preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def scale(data):\n",
    "    \"\"\" scale data (from begin_project_1b.py) \"\"\"\n",
    "    data_min = np.min(data, axis=0)\n",
    "    data_max = np.max(data, axis=0)\n",
    "    return (data - data_min)/(data_max - data_min)\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\" normalize data (from begin_project_1b.py) \"\"\"\n",
    "    data_mean = np.mean(data, axis=0)\n",
    "    data_std = np.std(data, axis=0)\n",
    "    return (data - data_mean)/data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Preprocessor class to load data, divide it into training and test set and then finally normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\" Preprocess class \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_data = None\n",
    "        self.y_data = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        return\n",
    "\n",
    "\n",
    "    def load_data(self, data_dir):\n",
    "        \"\"\" Load data from data_dir \"\"\"\n",
    "        temp_data = np.loadtxt(data_dir, delimiter=',')\n",
    "        self.x_data = temp_data[:,:8]\n",
    "        self.y_data = Y_data = (np.asmatrix(temp_data[:,-1])).transpose()\n",
    "\n",
    "    def divide_data(self, test_count, train_count):\n",
    "        \"\"\" Divide data into training and test set \"\"\"\n",
    "        div_count = (test_count * self.x_data.shape[0]) // (test_count + train_count)\n",
    "        self.test_x = self.x_data[:div_count]\n",
    "        self.test_y = self.y_data[:div_count]\n",
    "        self.train_x = self.x_data[div_count:]\n",
    "        self.train_y = self.y_data[div_count:]\n",
    "\n",
    "    def normalize_data(self):\n",
    "        \"\"\" Scale and Normalize test and train data \"\"\"\n",
    "        self.test_x = scale(self.test_x)\n",
    "        self.train_x = scale(self.train_x)\n",
    "\n",
    "        self.test_x = normalize(self.test_x)\n",
    "        self.train_x = normalize(self.train_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next will be the approximation model. Start with the utility function used by the Approximation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "\n",
    "FLOAT_X = theano.config.floatX\n",
    "\n",
    "def shuffle_data(samples, labels):\n",
    "    \"\"\" Shuffle data (from begin_project_1b.py) \"\"\"\n",
    "    idx = np.arange(samples.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    samples, labels = samples[idx], labels[idx]\n",
    "    return samples, labels\n",
    "\n",
    "def get_weigh(prev_layer, cur_layer):\n",
    "    \"\"\" get weigh \"\"\"\n",
    "    return theano.shared(np.random.randn(prev_layer, cur_layer)*0.01, FLOAT_X)\n",
    "\n",
    "def get_bias(cur_layer):\n",
    "    \"\"\" get weigh \"\"\"\n",
    "    return theano.shared(np.random.randn(cur_layer)*0.01, FLOAT_X)\n",
    "\n",
    "def get_fold_data(data, fold_num):\n",
    "    \"\"\" split the data into different fold \"\"\"\n",
    "    train = []\n",
    "    validation = []\n",
    "    for i in range(len(data)):\n",
    "        if i == fold_num:\n",
    "            validation = data[i]\n",
    "        else:\n",
    "            if len(train) == 0:\n",
    "                train = data[i]\n",
    "            else:\n",
    "                train = np.concatenate((train, data[i]))\n",
    "\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the class for Approximation. Using mini-batch gradient descent and k-fold cross validation for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "class Approximation:\n",
    "    \"\"\" Approximation Class \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "\n",
    "    def select_model(self, train_x, train_y, k_fold, epochs, batch_size, hidden_neurons, learning_rate):\n",
    "        \"\"\" select model using K-Fold cross validation \"\"\"\n",
    "        data_fold_x = []\n",
    "        data_fold_y = []\n",
    "\n",
    "        div_count = train_x.shape[0] // k_fold\n",
    "\n",
    "        for i in range(k_fold):\n",
    "            data_fold_x.append(train_x[i*div_count:(i+1)*div_count])\n",
    "            data_fold_y.append(train_y[i*div_count:(i+1)*div_count])\n",
    "\n",
    "        min_err = 1e+15\n",
    "        best_train_x = None\n",
    "        best_train_y = None\n",
    "\n",
    "        list_train_cost = []\n",
    "        list_test_cost = []\n",
    "        list_test_accuracy = []\n",
    "\n",
    "        for i in range(k_fold):\n",
    "            self.train_x, self.test_x = get_fold_data(data_fold_x, i)\n",
    "            self.train_y, self.test_y = get_fold_data(data_fold_y, i)\n",
    "            \n",
    "            self.create_model(hidden_neurons, learning_rate)\n",
    "            train_cost, test_cost, test_accuracy, new_err = self.train_model(epochs, batch_size)\n",
    "            \n",
    "            list_train_cost.append(train_cost)\n",
    "            list_test_cost.append(test_cost)\n",
    "            list_test_accuracy.append(test_accuracy)\n",
    "\n",
    "            if new_err < min_err:\n",
    "                min_err = new_err\n",
    "                best_train_x = self.train_x\n",
    "                best_train_y = self.train_y\n",
    "\n",
    "        self.train_x = best_train_x\n",
    "        self.train_y = best_train_y\n",
    "        return list_train_cost, list_test_cost, list_test_accuracy, min_err\n",
    "    \n",
    "    def create_model(self, list_of_neurons_on_hidden_layer, learning_rate):\n",
    "        \"\"\" Create model from list of neurons \"\"\"\n",
    "        no_features = self.train_x.shape[1] \n",
    "        \n",
    "        input_x = T.matrix('x') # data sample\n",
    "        expected_y = T.matrix('d') # desired output\n",
    "\n",
    "        alpha = theano.shared(learning_rate, FLOAT_X) \n",
    "\n",
    "        # initialize weights and biases for hidden layer(s) and output layer\n",
    "        weights = []\n",
    "        biases = []\n",
    "        prev_layer = no_features\n",
    "\n",
    "        for cur_layer in list_of_neurons_on_hidden_layer + [1]:\n",
    "            weights.append(get_weigh(prev_layer, cur_layer))\n",
    "            biases.append(get_bias(cur_layer))\n",
    "            prev_layer = cur_layer\n",
    "\n",
    "        # Define mathematical expression:\n",
    "        h_out = input_x\n",
    "        for w,b in zip(weights[:-1], biases[:-1]):\n",
    "            h_out = T.nnet.sigmoid(T.dot(h_out, w) + b)\n",
    "\n",
    "        h_out = T.dot(h_out, weights[-1]) + biases[-1]\n",
    "\n",
    "        cost = T.abs_(T.mean(T.sqr(expected_y - h_out)))\n",
    "        accuracy = T.mean(expected_y - h_out)\n",
    "\n",
    "        #define gradients\n",
    "        updates = []\n",
    "        grad = T.grad(cost, weights + biases)\n",
    "        grad_w = grad[:len(grad)//2]\n",
    "        grad_b = grad[len(grad)//2:]\n",
    "        for i in range(len(weights)):\n",
    "            updates.append([weights[i], weights[i] - alpha * grad_w[i]])\n",
    "            updates.append([biases[i], biases[i] - alpha * grad_b[i]])\n",
    "\n",
    "        self.train = theano.function(\n",
    "            inputs = [input_x, expected_y],\n",
    "            outputs = cost,\n",
    "            updates = updates,\n",
    "            allow_input_downcast=True\n",
    "            )\n",
    "\n",
    "        self.test = theano.function(\n",
    "            inputs = [input_x, expected_y],\n",
    "            outputs = [h_out, cost, accuracy],\n",
    "            allow_input_downcast=True\n",
    "            )\n",
    "\n",
    "    def train_model(self, epochs, batch_size):\n",
    "        \"\"\" train model based on self.train_x and self.train_y \"\"\"\n",
    "        train_cost = np.zeros(epochs)\n",
    "        test_cost = np.zeros(epochs)\n",
    "        test_accuracy = np.zeros(epochs)\n",
    "\n",
    "        min_error = 1e+15\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            self.train_x, self.train_y = shuffle_data(self.train_x, self.train_y)\n",
    "            train_cost[iter] = self.training_iter(batch_size)\n",
    "            _, test_cost[iter], test_accuracy[iter] = self.test_model()\n",
    "\n",
    "#             if iter%100 == 0:\n",
    "#                 print(\"Iter: %s\\n MSE: %s\\n Test Accuracy: %s\" %(iter, train_cost[iter], test_accuracy[iter]))\n",
    "#                 print(\"----------------------------------------------------------------------\")\n",
    "            \n",
    "            if test_cost[iter] < min_error:\n",
    "                min_error = test_cost[iter]\n",
    "        \n",
    "        return train_cost, test_cost, test_accuracy, min_error\n",
    "\n",
    "    def training_iter(self, batch_size):\n",
    "        cost = []\n",
    "\n",
    "        for i in range(0, len(self.train_x), batch_size):\n",
    "            end = i + batch_size\n",
    "            if end > len(self.train_x):\n",
    "                end = len(self.train_x)\n",
    "\n",
    "            train_x_batch = self.train_x[i:end]\n",
    "            train_y_batch = self.train_y[i:end]\n",
    "\n",
    "            cost.append(self.train(train_x_batch, train_y_batch))\n",
    "            \n",
    "        return np.mean(cost)\n",
    "\n",
    "    def test_model(self):\n",
    "        \"\"\" test model using independent test data \"\"\"\n",
    "        return self.test(self.test_x, self.test_y)\n",
    "\n",
    "    def set_x_train(self, train_x):\n",
    "        \"\"\" self.train_x setter \"\"\"\n",
    "        self.train_x = train_x\n",
    "\n",
    "    def set_y_train(self, train_y):\n",
    "        \"\"\" self.train_y setter \"\"\"\n",
    "        self.train_y = train_y\n",
    "\n",
    "    def set_x_test(self, test_x):\n",
    "        \"\"\" self.test_x setter \"\"\"\n",
    "        self.test_x = test_x\n",
    "\n",
    "    def set_y_test(self, test_y):\n",
    "        \"\"\" self.test_y setter \"\"\"\n",
    "        self.test_y = test_y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last is the visualization class. It will support plotting multiple graph in one figure (as long as using the same x range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(title, x_label, y_label, x_val, y_vals, data_labels, save_fig=\"\", show_graph=True):\n",
    "    \"\"\" Plot graphs using matplotlib \"\"\"\n",
    "    plt.figure()\n",
    "\n",
    "    for i in range(len(data_labels)):\n",
    "        plt.plot(x_val, y_vals[i], label=data_labels[i])\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    if save_fig != \"\":\n",
    "        plt.savefig(save_fig)\n",
    "\n",
    "    if show_graph:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Design a 3-layer feedforward neural network consisting of a hidden-layer of 30 neurons. Use mini-batch gradient descent (with batch size of 32 and learning rate = 10−4) to train the network. Use up to about 1000 epochs for this problem.\n",
    "    a. Plot the training error against number of epochs for the 3-layer network.\n",
    "    b. Plot the final test errors of prediction by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Start Question 1\")\n",
    "    np.random.seed(10)\n",
    "\n",
    "    k_fold = 5\n",
    "    hidden_neurons = [30]\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-4\n",
    "    epochs = 1000\n",
    "    data_dir = \"housing_data/cal_housing.data\"\n",
    "\n",
    "    # Preprocessing: Load data\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.load_data(data_dir)\n",
    "    preprocessor.divide_data(3, 7)\n",
    "    preprocessor.normalize_data()\n",
    "\n",
    "    nn = Approximation()\n",
    "\n",
    "    if k_fold > 0:\n",
    "        list_train_cost, list_test_cost, list_test_accuracy, min_err = nn.select_model(\n",
    "            train_x = preprocessor.train_x,\n",
    "            train_y = preprocessor.train_y,\n",
    "            k_fold = k_fold,\n",
    "            epochs = epochs,\n",
    "            batch_size = batch_size,\n",
    "            hidden_neurons = hidden_neurons,\n",
    "            learning_rate = learning_rate\n",
    "        )\n",
    "        # TODO:\n",
    "        # Plot K-fold graphs\n",
    "\n",
    "    else:\n",
    "        nn.set_x_train(preprocessor.train_x)\n",
    "        nn.set_y_train(preprocessor.train_y)\n",
    "    \n",
    "    nn.set_x_test(preprocessor.test_x)\n",
    "    nn.set_y_test(preprocessor.test_y)\n",
    "    nn.create_model(hidden_neurons, learning_rate)\n",
    "    train_cost, test_cost, accuracy, min_err = nn.train_model(epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Plot training error against number of epoch\n",
    "    # Plot test error of prediction against number of epoch\n",
    "    plot_graph(\n",
    "        title='Training and Test Errors at Alpha = %.3f'%learning_rate,\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"MSE\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=[train_cost, test_cost],\n",
    "        data_labels=[\"train\", \"test\"],\n",
    "    )\n",
    "\n",
    "    # Plot accuracy against number of epoch\n",
    "    plot_graph(\n",
    "        title=\"Test Accuracy\",\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"Accuracy\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=[accuracy],\n",
    "        data_labels=[\"Test Accuracy\"],\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the optimal learning rate for the 3-layer network designed. Set this as the learning rate in first hidden layer for the rest of the experiments.\n",
    "    a. Plot the training errors and validation errors against number of epochs for the 3-layer network for different learning rates. Limit the search space to: {10−3, 0.5 × 10−3, 10−4, 0.5 × 10−4, 10−5}\n",
    "    b. Plot the test errors against number of epochs for the optimum learning rate.\n",
    "    c. State the rationale behind selecting the optimal learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Start Question 2\")\n",
    "    np.random.seed(10)\n",
    "\n",
    "    k_fold = 5\n",
    "    hidden_neurons = [30]\n",
    "    batch_size = 32\n",
    "    learning_rate_list = [1e-3, 0.5 * 1e-3, 1e-4, 0.5 * 1e-4, 1e-5]\n",
    "    epochs = 1000\n",
    "    data_dir = \"housing_data/cal_housing.data\"\n",
    "\n",
    "    # Preprocessing: Load data\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.load_data(data_dir)\n",
    "    preprocessor.divide_data(3, 7)\n",
    "    preprocessor.normalize_data()\n",
    "\n",
    "    list_train_cost = []\n",
    "    list_test_cost = []\n",
    "    list_test_accuracy = []\n",
    "    nn = Approximation()\n",
    "\n",
    "    for learning_rate in learning_rate_list:\n",
    "        if k_fold > 0:\n",
    "            nn.select_model(\n",
    "                train_x = preprocessor.train_x,\n",
    "                train_y = preprocessor.train_y,\n",
    "                k_fold = k_fold,\n",
    "                epochs = epochs,\n",
    "                batch_size = batch_size,\n",
    "                hidden_neurons = hidden_neurons,\n",
    "                learning_rate = learning_rate\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            nn.set_x_train(preprocessor.train_x)\n",
    "            nn.set_y_train(preprocessor.train_y)\n",
    "        \n",
    "        nn.set_x_test(preprocessor.test_x)\n",
    "        nn.set_y_test(preprocessor.test_y)\n",
    "        nn.create_model(hidden_neurons, learning_rate)\n",
    "        train_cost, test_cost, test_accuracy, min_err = nn.train_model(epochs=epochs, batch_size=batch_size)\n",
    "        list_train_cost.append(train_cost)\n",
    "        list_test_cost.append(test_cost)\n",
    "        list_test_accuracy.append(test_accuracy)\n",
    "\n",
    "    # Plot training error against number of epoch\n",
    "    plot_graph(\n",
    "        title='Train Errors for each Alpha',\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"MSE\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_train_cost,\n",
    "        data_labels=learning_rate_list,\n",
    "    )\n",
    "\n",
    "    # Plot test error of prediction against number of epoch\n",
    "    plot_graph(\n",
    "        title='Test Errors for each Alpha',\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"MSE\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_test_cost,\n",
    "        data_labels=learning_rate_list,\n",
    "    )\n",
    "\n",
    "    # Plot accuracy against number of epoch\n",
    "    plot_graph(\n",
    "        title=\"Test Accuracy\",\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"Accuracy\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_test_accuracy,\n",
    "        data_labels=learning_rate_list,\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find the optimal number of hidden neurons for the 3-layer network designed.\n",
    "    a. Plot the training errors against number of epochs for the 3-layer network for different hidden-layer neurons. Limit search space to:{20,30,40,50,60}.\n",
    "    b. Plot the test errors against number of epochs for the optimum number of hiddenlayer neurons.\n",
    "    c. State the rationale behind selecting the optimal number of hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Start Question 3\")\n",
    "    np.random.seed(10)\n",
    "\n",
    "    k_fold = 5\n",
    "    hidden_neuron_list = [[20], [30], [40], [50], [60]]\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-4\n",
    "    epochs = 1000\n",
    "    data_dir = \"housing_data/cal_housing.data\"\n",
    "\n",
    "    # Preprocessing: Load data\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.load_data(data_dir)\n",
    "    preprocessor.divide_data(3, 7)\n",
    "    preprocessor.normalize_data()\n",
    "\n",
    "    list_train_cost = []\n",
    "    list_test_cost = []\n",
    "    list_test_accuracy = []\n",
    "    nn = Approximation()\n",
    "\n",
    "    for hidden_neurons in hidden_neuron_list:\n",
    "        if k_fold > 0:\n",
    "            nn.select_model(\n",
    "                train_x = preprocessor.train_x,\n",
    "                train_y = preprocessor.train_y,\n",
    "                k_fold = k_fold,\n",
    "                epochs = epochs,\n",
    "                batch_size = batch_size,\n",
    "                hidden_neurons = hidden_neurons,\n",
    "                learning_rate = learning_rate\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            nn.set_x_train(preprocessor.train_x)\n",
    "            nn.set_y_train(preprocessor.train_y)\n",
    "        \n",
    "        nn.set_x_test(preprocessor.test_x)\n",
    "        nn.set_y_test(preprocessor.test_y)\n",
    "        nn.create_model(hidden_neurons, learning_rate)\n",
    "        train_cost, test_cost, test_accuracy, min_err = nn.train_model(epochs=epochs, batch_size=batch_size)\n",
    "        list_train_cost.append(train_cost)\n",
    "        list_test_cost.append(test_cost)\n",
    "        list_test_accuracy.append(test_accuracy)\n",
    "\n",
    "    # Plot training error against number of epoch\n",
    "    plot_graph(\n",
    "        title='Train Errors for each Neuron',\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"MSE\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_train_cost,\n",
    "        data_labels=hidden_neuron_list,\n",
    "    )\n",
    "\n",
    "    # Plot test error of prediction against number of epoch\n",
    "    plot_graph(\n",
    "        title='Test Errors for each Neuron',\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"MSE\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_test_cost,\n",
    "        data_labels=hidden_neuron_list,\n",
    "    )\n",
    "\n",
    "    # Plot accuracy against number of epoch\n",
    "    plot_graph(\n",
    "        title=\"Test Accuracy\",\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"Accuracy\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_test_accuracy,\n",
    "        data_labels=hidden_neuron_list,\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Design a four-layer neural network and a five-layer neural network, with the first hidden layer having number of neurons found in step (3) and other hidden layers having 20 neurons each. Use a learning rate = 10−4. Plot the test errors of the 4-layer network and 5-layer network, and compare them with that of the 3-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from housing_preprocessing.preprocess import Preprocessor\n",
    "from housing_training.approximation import Approximation\n",
    "from housing_visualization.visualization import plot_graph\n",
    "\n",
    "def main():\n",
    "    print(\"Start Question 4\")\n",
    "    np.random.seed(10)\n",
    "\n",
    "    k_fold = 0\n",
    "    hidden_neuron_list = [[40], [40, 20], [40, 20, 20]]\n",
    "    batch_size = 4048\n",
    "    learning_rate = 1e-4\n",
    "    epochs = 1000\n",
    "    data_dir = \"housing_data/cal_housing.data\"\n",
    "\n",
    "    # Preprocessing: Load data\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.load_data(data_dir)\n",
    "    preprocessor.divide_data(3, 7)\n",
    "    preprocessor.normalize_data()\n",
    "\n",
    "    list_train_cost = []\n",
    "    list_test_cost = []\n",
    "    list_test_accuracy = []\n",
    "    nn = Approximation()\n",
    "\n",
    "    for hidden_neurons in hidden_neuron_list:\n",
    "        if k_fold > 0:\n",
    "            nn.select_model(\n",
    "                train_x = preprocessor.train_x,\n",
    "                train_y = preprocessor.train_y,\n",
    "                k_fold = k_fold,\n",
    "                epochs = epochs,\n",
    "                batch_size = batch_size,\n",
    "                hidden_neurons = hidden_neurons,\n",
    "                learning_rate = learning_rate\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            nn.set_x_train(preprocessor.train_x)\n",
    "            nn.set_y_train(preprocessor.train_y)\n",
    "        \n",
    "        nn.set_x_test(preprocessor.test_x)\n",
    "        nn.set_y_test(preprocessor.test_y)\n",
    "        nn.create_model(hidden_neurons, learning_rate)\n",
    "        train_cost, test_cost, test_accuracy, min_err = nn.train_model(epochs=epochs, batch_size=batch_size)\n",
    "        list_train_cost.append(train_cost)\n",
    "        list_test_cost.append(test_cost)\n",
    "        list_test_accuracy.append(test_accuracy)\n",
    "\n",
    "    # Plot training error against number of epoch\n",
    "    plot_graph(\n",
    "        title='Train Errors for each Neuron',\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"MSE\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_train_cost,\n",
    "        data_labels=hidden_neuron_list,\n",
    "    )\n",
    "\n",
    "    # Plot test error of prediction against number of epoch\n",
    "    plot_graph(\n",
    "        title='Test Errors for each Neuron',\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"MSE\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_test_cost,\n",
    "        data_labels=hidden_neuron_list,\n",
    "    )\n",
    "\n",
    "    # Plot accuracy against number of epoch\n",
    "    plot_graph(\n",
    "        title=\"Test Accuracy\",\n",
    "        x_label=\"Epochs\",\n",
    "        y_label=\"Accuracy\",\n",
    "        x_val=range(epochs),\n",
    "        y_vals=list_test_accuracy,\n",
    "        data_labels=hidden_neuron_list,\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
